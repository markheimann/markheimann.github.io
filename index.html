---
layout: default
title: Mark Heimann
---
<div class="blurb">
  <p>
    I am a PhD candidate in the computer science department at the University of Michigan, where I consider myself fortunate to be a member of the <a href="http://web.eecs.umich.edu/~dkoutra/group/index.html">GEMS Lab</a> and advised by <a href="http://web.eecs.umich.edu/~dkoutra">Danai Koutra</a>. 
  </p>

<p>
    Specifically, my current research interests involve representation learning in networks, where I am developing methods to learn vector embeddings of nodes that can be used for multi-network problems such as network alignment.  I am interested in the connection between node representation learning methods based on deep learning-inspired language modeling techniques, matrix factorization, and low-rank matrix approximation.  I also have broader interests in develping scalable methods for various data mining tasks involving network data.  
</p> 

<p>
    I have an eclectic set of <a href="other-activities.html">other interests</a> ranging from competitive (high-level) chess and (novice-level) powerlifting to music and amateur baking.  Sometimes I manage to combine them with my work; for instance, I am very interested in creating artificial intelligence technology for music analysis and performance, as well as the design and use of electronic effects especially for augmenting acoustic instruments.  Most of the time, though, they exist in the space left for them by life as a PhD student, but I'm always happy to talk about them.  
</p>


</div><!-- /.blurb -->

<h2>Research Projects</h2>
<div class="blurb">
<ul>
  <li><b>Multi-Network Representation Learning: </b> Methods to learn representations for nodes in graphs have exploded in popularity as techniques from deep learning have been shown to be extensible to the network setting and have led to state-of-the-art results on single graphs.  We show that most existing methods have weaknesses when applied to multiple graphs, because the criteria they optimize only make sense in a single graph.  We develop a new method for representation learning, Structural Matrix Factorization that a) preserves node similarities based on structural identity that can be compared in multiple graphs, and b) omits the variance-inducing context sampling with random walks that many competing methods use.   Our main framework, REGAL (REpresentation learning for Graph ALignment) iteratively alternates between learning alignment information by matching nodes with similar embeddings and learning embeddings that preserve similarities between nodes thought to align. 

  	<img src="assets/project_figs/REGAL.png" class = "project_figs">

  	&nbsp;
  	&nbsp;
  	&nbsp;
  	&nbsp;


  <li><b>Fast Network Alignment: </b> We develop HashAlign, a flexible framework for aligning graphs quickly by using locality-sensitive hashing to avoid comparing all possible combinations of nodes.  Instead, with high probability we only compare the most similar nodes based on feature vectors for which we propose sensible construction methods.  This framework is flexible, as it can be easily extended to graphs where node and/or edge attributes are known; if known, they can be incorporated into the nodes' feature vectors. 

  <img src="assets/project_figs/HashAlign.png" class = "project_figs"> 

  &nbsp;
  &nbsp;
  &nbsp;
  &nbsp;

  <li><b>Solving Large-Scale Linear Systems: </b> We develop FLOWR, a fast "flow"-based methods for solving network problems that can be expressed as linear systems, including Random Walk with Restart (RWR), in a distributed setting.  Such systems, we show, can be solved in a divide-and-conquer fashion by breaking them into subproblems (smaller clusters of the original network) such that the communication between subproblems (vertices adjacent to vertices in other clusters, or "flow") is minimized.  Furthermore, our method can leverage the benefit of overlapping clusters, which makes individual clusters larger, but minimizes the amount of communication across clusters.

  	<img src="assets/project_figs/FLOWR.png" class = "project_figs">

  	&nbsp;
  	&nbsp;
  	&nbsp;
  	&nbsp;
</ul>

</div>

<h2>Publications</h2>
<div class="blurb">
<ul>
	<li> Wei Lee, Mark Heimann, Shengjie Pan, Kuan-Yu Chen, and Danai Koutra.  Fast Multi-Network Alignment with Locality-Sensitive Hashing.  Under Review, PAKDD 2018.
	<li> Mark Heimann, Haoming Shen, and Danai Koutra.  Multi-Network Representation Learning with Applications to Network Alignment. Under Review, SDM 2018.
	<li> Yujun Yan, Mark Heimann, Di Jin, and Danai Koutra.  Fast Flow-based Methods for Solving Linear Systems in a Distributed Multi-query Setting.  Under Review, SDM 2018.
<li> Mark Heimann and Danai Koutra. <a href="http://www.mlgworkshop.org/2017/paper/MLG2017_paper_26.pdf">On Generalizing Neural Node Embedding Methods to Multi-Network Problems</a>. In Proceedings of the 13th International Workshop on
Mining and Learning with Graphs (MLG), 2017.
</ul>
</div>
